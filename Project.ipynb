{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIN-413 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import gaussian_kde\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "# Useful function to display dataframe + display shape\n",
    "def disp(df, n=5, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    display(df.head(n))\n",
    "    print(df.shape)\n",
    "\n",
    "def save_large_tables(dataframe, path):\n",
    "    c = dataframe.columns.to_list()\n",
    "    dataframe[c[:len(c)//2]].to_latex(f\"{path}_part_1.tex\")\n",
    "    dataframe[c[len(c)//2:]].to_latex(f\"{path}_part_2.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", skiprows=[0])\n",
    "\n",
    "# Parse the 'time' column to datetime if it's not already in that format\n",
    "data['time'] = pd.to_datetime(data['time'], errors='coerce')\n",
    "\n",
    "# Convert all other columns to numeric values, assuming they represent returns\n",
    "for col in data.columns[1:]:\n",
    "    data[col] = pd.to_numeric(data[col], errors='coerce')\n",
    "\n",
    "# Set the index\n",
    "data.set_index(\"time\", drop = True, inplace = True)\n",
    "\n",
    "# Data overview\n",
    "print(\"Data overview\")\n",
    "display(data)\n",
    "save_large_tables(data, \"tables/part1_returns_raw\")\n",
    "# data.to_latex(\"tables/part1_returns_raw.tex\")\n",
    "\n",
    "# Data description\n",
    "print(\"Data description\")\n",
    "display(data.info())\n",
    "\n",
    "# Create list of the names of the crypto / indices\n",
    "cryptos = ['ADA', 'BCH', 'BTC', 'DOGE', 'ETH', 'LINK', 'LTC', 'MANA', 'XLM', 'XRP']\n",
    "indices = ['SPXT', 'XCMP', 'USSOC', 'VIX']\n",
    "\n",
    "# Important dates\n",
    "datePP = datetime.strptime(\"2021/09/11\", \"%Y/%m/%d\")\n",
    "dateTr = datetime.strptime(\"2022/11/21\", \"%Y/%m/%d\")\n",
    "dateRec = datetime.strptime(\"2024/03/07\", \"%Y/%m/%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics\n",
    "print(\"Descriptive statistics\")\n",
    "descriptive_stats = data.describe()\n",
    "display(descriptive_stats)\n",
    "# descriptive_stats.to_latex(\"tables/part1_descriptive_stats.tex\")\n",
    "save_large_tables(descriptive_stats, \"tables/part1_descriptive_stats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: correlation over the assets\n",
    "plt.figure(figsize=(14, 10))\n",
    "correlation_matrix = data.corr() \n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')\n",
    "plt.savefig(\"figures/part1_correlation_heatmap\")\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling correlations for crypto and traditional assets\n",
    "window_size = 30  # 30 days rolling window\n",
    "\n",
    "# Iterate over each cryptocurrency\n",
    "for crypto in cryptos: # ['ADA', 'BCH', 'BTC', 'DOGE', 'ETH', 'LINK', 'LTC', 'MANA', 'XLM', 'XRP']\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Iterate over each traditional asset\n",
    "    for trad in indices: # ['SPXT', 'XCMP', 'USSOC', 'VIX']\n",
    "        # Calculate rolling correlation\n",
    "        rolling_corr = data[crypto].rolling(window=window_size).corr(data[trad])\n",
    "        \n",
    "        # Ensure the index is a flat array for plotting\n",
    "        rolling_corr_index = rolling_corr.index.values.flatten()\n",
    "        fig.add_trace(go.Scatter(x = rolling_corr_index, y=rolling_corr.values, mode='lines', name=f'{crypto} vs {trad}'))\n",
    "    \n",
    "    fig.update_layout(title=f'Rolling Correlation of {crypto} with Traditional Assets',\n",
    "                 xaxis_title='Time',\n",
    "                 yaxis_title='Correlation')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual density of the assets\n",
    "\n",
    "# Create generator to have the name of all securities\n",
    "def give_col(data):\n",
    "    for col in data.columns:\n",
    "        yield col\n",
    "\n",
    "def plot_sub_hist(df, title=\"Distribution of returns of the different securities\", show=True, rows_nb = 7, cols_nb = 2, save=False, save_title=\"figures/part1b_distribution_returns\"):\n",
    "    column_generator = give_col(df)\n",
    "\n",
    "\n",
    "    rows = rows_nb # Number of rows\n",
    "    cols = cols_nb # Number of columns\n",
    "\n",
    "    fig = make_subplots(rows=rows, cols=cols, subplot_titles=list(give_col(df)))\n",
    "    # Loop through data and add traces\n",
    "    for col in range(cols):\n",
    "        col += 1\n",
    "        for row in range(rows):\n",
    "            row += 1\n",
    "            sec = next(column_generator)\n",
    "            trace = go.Histogram(\n",
    "                x=df[sec], \n",
    "                nbinsx=300,\n",
    "                name = f\"{sec}\")\n",
    "            fig.append_trace(trace, row, col)\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=rows*200, \n",
    "        width=1500, \n",
    "        title_text=title)\n",
    "    \n",
    "    if show:\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "plot_sub_hist(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Marginal density plots for each asset\n",
    "plt.figure(figsize=(14, 7))\n",
    "for column in data.columns[1:]:         # Skipping 'time' column\n",
    "    sns.kdeplot(data[column].dropna(), fill=True, label=column)  # Drop NA values for kde plot\n",
    "plt.savefig(\"figures/part1_marginal_density_plot.png\")\n",
    "plt.title('Marginal Density Plots for All Assets')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "fig = px.histogram(\n",
    "    data_frame=data, \n",
    "    histnorm='density')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not sure that this is relevant - Could be removed ? # Agu: It is a little bit unreadable like this. Maybe we can separate the plots? \n",
    "fig = px.scatter(\n",
    "    data,\n",
    "    y = cryptos, x = data.index,\n",
    "    color_continuous_scale='picnic',\n",
    "    width = 1600, height = 600,\n",
    "    title = 'Daily values of cryptos against USD',\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.a) Log returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__TO BE REMOVED OR UPDATED WITH LATEST VERSION FROM THE REPORT__*\n",
    "\n",
    "For this analysis, log returns should be used. The marginal distributions of crypto assets typically exhibit high kurtosis and skewness, suggesting a non-normal distribution. Log returns, which normalize and stabilize variance, are more suitable for the statistical models used in risk-based portfolio optimization, particularly when dealing with the high volatility of cryptocurrencies. Furthermore, cryptocurrency prices can fluctuate rapidly since they are traded continuously. Log returns, which consider compounding effects, provide a more accurate representation of returns. This choice aligns with Meucci's insights on the appropriateness of log returns for multiperiod investment horizons and assets with volatile behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tranform values into log\n",
    "data_log = data.apply(lambda x: np.log(x+1))\n",
    "# data_log.to_latex(\"tables/part1a_log_returns.tex\")\n",
    "save_large_tables(data_log, \"tables/part1a_log_returns\")\n",
    "print(\"Log returns\")\n",
    "disp(data)\n",
    "print(\"Missing data:\")\n",
    "disp(data.isna().sum(), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual distribution of log returns\n",
    "plot_sub_hist(data_log, title = \"Distribution of log returns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.b) Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using linear returns - for information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential outliers\n",
    "potential_outliers = data[(np.abs(data - data.mean()) > (3 * data.std())).any(axis=1)]\n",
    "# potential_outliers.to_latex(\"tables/part1b_potential_outliers.tex\")\n",
    "save_large_tables(potential_outliers, \"tables/part1b_potential_outliers\")\n",
    "disp(potential_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR for each column except 'time' and identify outliers\n",
    "Q1 = data.quantile(0.25)\n",
    "Q3 = data.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = (data < (Q1 - 3 * IQR)) | (data > (Q3 + 3 * IQR))\n",
    "\n",
    "# Calculate the total number of outliers identified\n",
    "total_outliers = outliers.sum().sum()  # Summing up all True values for outliers\n",
    "\n",
    "# Handle outliers: Replace them with NaN for columns other than 'time'\n",
    "data_cleaned = data.copy()\n",
    "data_cleaned = data.mask(outliers)\n",
    "data_cleaned.to_latex(\"tables/part1b_data_clean.tex\")\n",
    "save_large_tables(data_cleaned,\"tables/part1b_data_clean\")\n",
    "\n",
    "# Fill NaN values using forward fill, then backward fill for any remaining at the start\n",
    "data_cleaned.fillna(method='ffill', inplace=True)\n",
    "data_cleaned.fillna(method='bfill', inplace=True)\n",
    "\n",
    "# Print the total number of outliers\n",
    "print(\"Total number of outliers identified:\", total_outliers)\n",
    "\n",
    "disp(outliers)\n",
    "\n",
    "print(\"\\nClean data\")\n",
    "disp(data_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__TO BE REMOVED OR UPDATED WITH LATEST VERSION FROM THE REPORT__*\n",
    "\n",
    "We conducted outlier detection using two distinct statistical methods due to the unique characteristics of financial data, especially in highly volatile markets like those of cryptocurrencies.\n",
    "\n",
    "Standard Deviation Method: The first approach involved identifying outliers as those data points lying beyond three standard deviations from the mean. While commonly used in many fields for outlier detection, this method proved overly sensitive for our dataset. It flagged a large number of data points as outliers, which upon further examination, were within plausible ranges for cryptocurrency returns. This high sensitivity is attributed to the large standard deviations characteristic of these markets, suggesting that the method might not be suitable for data with high volatility.\n",
    "\n",
    "Interquartile Range (IQR) Method: Given the limitations observed with the standard deviation approach, we shifted to the IQR method. The IQR technique is less affected by extreme values and provides a more robust measure for identifying outliers in skewed distributions. Initially, a commonly used multiplier of 1.5 was applied to the IQR to define the outliers, which still resulted in a high number of outliers. Adjusting the multiplier to 3 significantly reduced the count to 1361 outliers, offering a more reasonable quantification (relatively to our original dataset size) that reflects true anomalous behaviors without discarding legitimate data variations.\n",
    "\n",
    "Handling Identified Outliers: After identifying outliers with the adjusted IQR method, we handled these by initially replacing them with NaN values to prevent any distortion in the subsequent analysis. To maintain the continuity and integrity of our time series data, these NaN values were then addressed using forward filling (ffill), which propagates the last valid observation forward. Where forward filling was insufficient (e.g., at the start of the data series), backward filling (bfill) was employed to ensure no data point was left unfilled. This approach helps in preserving the temporal structure of the data while minimizing the impact of extreme value fluctuations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using log returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate log returns for all assets\n",
    "data_log_returns = np.log1p(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify potential outliers\n",
    "potential_outliers_logreturns = data_log_returns[(np.abs(data_log_returns - data_log_returns.mean()) > (3 * data_log_returns.std())).any(axis=1)]\n",
    "# potential_outliers.to_latex(\"tables/part1b_potential_outliers.tex\")\n",
    "save_large_tables(potential_outliers_logreturns, \"tables/part1b_potential_outliers_std_method_logreturn\")\n",
    "disp(potential_outliers_logreturns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the IQR for log returns\n",
    "Q1_log = data_log_returns.quantile(0.25)\n",
    "Q3_log = data_log_returns.quantile(0.75)\n",
    "IQR_log = Q3_log - Q1_log\n",
    "\n",
    "# Identify outliers using the IQR method for log returns\n",
    "outliers_log = (data_log_returns < (Q1_log - 3 * IQR_log)) | (data_log_returns > (Q3_log + 3 * IQR_log))\n",
    "\n",
    "# Calculate the total number of outliers\n",
    "total_outliers_log = outliers_log.sum().sum()\n",
    "\n",
    "# Handle outliers by replacing them with NaN and then forward-filling\n",
    "data_log_returns_masked = data_log_returns.mask(outliers_log)\n",
    "data_log_returns_filled = data_log_returns_masked.fillna(method='ffill').fillna(method='bfill')\n",
    "data_log_returns_filled.to_latex(\"tables/part1b_data_clean_logreturns.tex\")\n",
    "save_large_tables(data_log_returns_filled, \"tables/part1b_data_clean_logreturns\")\n",
    "\n",
    "print(\"Total number of outliers identified:\", total_outliers_log)\n",
    "disp(data_log_returns_filled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's rescale the marginal density plots with log returns\n",
    "fig = px.histogram(\n",
    "    data_frame=data_log_returns, \n",
    "    histnorm='density')\n",
    "fig.update_layout(title=f'Rescaled Marginal Density Plots for Log Returns',\n",
    "                 xaxis_title='Log Returns',\n",
    "                 yaxis_title='Density')\n",
    "fig.show()\n",
    "fig.write_image(\"figures/part1b_rescaled_marginal_density_logreturns.png\")\n",
    "\n",
    "\n",
    "\n",
    "# Now, we'll conduct statistical tests for skewness and kurtosis to quantify the shape of the distribution.\n",
    "skewness = data_log_returns.skew().sort_values()\n",
    "skewness.to_latex(\"tables/part1b_skewness.tex\")\n",
    "kurtosis = data_log_returns.kurtosis().sort_values()\n",
    "kurtosis.to_latex(\"tables/part1b_kurtosis.tex\")\n",
    "\n",
    "\n",
    "# Display skewness and kurtosis values\n",
    "print(\"Skewness of Log Returns:\")\n",
    "print(skewness)\n",
    "print(\"\\nKurtosis of Log Returns:\")\n",
    "print(kurtosis)\n",
    "\n",
    "# Next, we'll create a Q-Q plot for one of the assets to visually inspect its distribution against a normal distribution.\n",
    "# Here we use BTC as an example. we would repeat this for other assets or iterate over all.\n",
    "plt.figure(figsize=(8, 8))\n",
    "stats.probplot(data_log_returns['BTC'].dropna(), dist=\"norm\", plot=plt)\n",
    "plt.savefig(\"figures/part1b_QQ_plot_BTC_log\")\n",
    "plt.title('Q-Q Plot for BTC Log Returns')\n",
    "plt.show()\n",
    "\n",
    "# Lastly, we'll plot a box plot to visualize the spread and outliers in the log returns.\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.boxplot(data=data_log_returns)\n",
    "plt.xlabel('Assets')\n",
    "plt.ylabel('Log Returns')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"figures/part1b_box_plot\")\n",
    "plt.title('Box Plot for Log Returns')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.c) Total returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__TO BE REMOVED OR UPDATED WITH LATEST VERSION FROM THE REPORT__*\n",
    "\n",
    "Does it Matter in General?\n",
    "Yes, significantly because of 2 aspects:\n",
    "\n",
    "- Accuracy in Returns: Total return indices, which include dividends, provide a fuller picture of the true returns an investor would receive, as they reflect both price changes and income from dividends.\n",
    "- Benchmarking and Relevance: They offer a more realistic benchmark for assessing investment performance and are particularly relevant for investors who reinvest dividends.\n",
    "\n",
    "For the Risk-Based Portfolio Optimizations in This Study?\n",
    "Indeed, it is also crucial:\n",
    "\n",
    "- Impact on Risk and Reward: The inclusion of dividends affects both the risk and return calculations, altering the risk-return profile used in portfolio optimizations.\n",
    "- Influence on Diversification: These indices might show different behaviors compared to price-only indices, particularly affecting diversification strategies due to changes in asset correlations.\n",
    "- Optimization Outcomes: Total return indices can shift the efficient frontier upward, leading to different optimal portfolio compositions and weightings.\n",
    "\n",
    "In a nutshell, using total return indices like SPXT and XCMP is essential for realistic and effective portfolio management, especially in a study focusing on risk-based optimization. This approach ensures that all components of returns are considered, facilitating more accurate decision-making in portfolio construction and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.d) Weekend activity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*__TO BE REMOVED OR UPDATED WITH LATEST VERSION FROM THE REPORT__*\n",
    "\n",
    "Observation and Impact:\n",
    "For traditional assets like SPXT and XCMP, zero returns over weekends are noted because these markets are closed, whereas cryptocurrencies trade 24/7, leading to non-zero returns every day. This discrepancy can distort correlation calculations between traditional and crypto assets, as weekends introduce artificial stability into the returns of traditional assets.\n",
    "\n",
    "This is a problem for statistical analyses, especially when calculating correlations or conducting any comparative time series analysis. The zero returns can artificially lower the volatility and skew the correlation metrics between traditional and crypto assets.\n",
    "\n",
    "Suggested Solution:\n",
    "\n",
    "- Synchronize the Data: One approach is to adjust the dataset so that returns for traditional assets are only calculated on days when those markets are open (i.e., exclude weekends). Alternatively, calculate weekly returns instead of daily returns to ensure comparability across all assets.\n",
    "- Use Imputation: For correlation purposes and other time-sensitive analyses, impute weekend returns for traditional assets using interpolation or forward-filling from the nearest non-zero data point. This maintains data continuity without introducing significant bias.\n",
    "This methodological adjustment helps align the data temporally, providing more accurate and meaningful analytical outcomes, particularly in mixed-asset portfolios like those including both traditional and crypto assets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make sure we have correct data\n",
    "DATA_LOG_RETURN = data_log_returns_filled\n",
    "RETURNS_PP = data_cleaned.copy().loc[:datePP]\n",
    "RETURNS_TR = data_cleaned.copy().loc[datePP:dateTr]\n",
    "# display(RETURNS_PP.mean())\n",
    "# display(RETURNS_TR.mean())\n",
    "\n",
    "synthetic_pricesPP = (1 + RETURNS_PP).cumprod()\n",
    "synthetic_pricesTR = (1 + RETURNS_TR).cumprod()\n",
    "\n",
    "display(synthetic_pricesPP)\n",
    "\n",
    "synthetic_pricesPP.plot(figsize=(16,6))\n",
    "plt.show()\n",
    "display(synthetic_pricesTR)\n",
    "synthetic_pricesTR.plot(figsize=(16,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.a) Equally weigthed portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "weights_EW = 1 / len(DATA_LOG_RETURN.columns)\n",
    "weights_EW_list = np.array([weights_EW for i in range(len(DATA_LOG_RETURN.columns))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_EW_log = DATA_LOG_RETURN.copy()\n",
    "portfolio_EW_log[\"Return_EW\"] = portfolio_EW_log.apply(lambda row: row.sum(), axis=1) * weights_EW\n",
    "print(\"Marginal contribution EW portfolio\")\n",
    "disp(portfolio_EW_log)\n",
    "portfolio_EW_log.to_latex(\"tables/part2a_EW_marginal_contribution.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expected return and std EW portfolio\n",
    "expected_returns_PP = weights_EW_list @ np.array(RETURNS_PP.mean().to_list()).T\n",
    "expected_returns_TR = weights_EW_list @ np.array(RETURNS_TR.mean().to_list()).T\n",
    "\n",
    "variance_EW_PP = (weights_EW_list @ RETURNS_PP.cov() @ weights_EW_list.T)\n",
    "variance_EW_TR = (weights_EW_list @ RETURNS_TR.cov() @ weights_EW_list.T)\n",
    "\n",
    "\n",
    "# print(\"Expected return:\\t\", round(expected_returns, 4))\n",
    "# print(\"Standard deviation:\\t\", round(variance_EW**0.5, 4))\n",
    "\n",
    "print(f\"Equally weighted portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(expected_returns_PP, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(variance_EW_PP ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"Equally weighted portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(expected_returns_TR, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(variance_EW_TR ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the returns of the portfolio EW\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(x = portfolio_EW_log.loc[dateTr:dateRec].index, \n",
    "                 y=portfolio_EW_log.loc[dateTr:dateRec].Return_EW, \n",
    "                 mode='lines', \n",
    "                 name=f'Before {datetime.strftime(dateRec,\"%Y/%m/%d\")} (dateRec)',\n",
    "                 ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x = portfolio_EW_log.loc[datePP:dateTr].index, \n",
    "                 y=portfolio_EW_log.loc[datePP:dateTr].Return_EW, \n",
    "                 mode='lines', \n",
    "                 name=f'Before {datetime.strftime(dateTr,\"%Y/%m/%d\")} (dateTr)',\n",
    "                 ))\n",
    "\n",
    "fig.add_trace(go.Scatter(x = portfolio_EW_log.loc[:datePP].index, \n",
    "                 y=portfolio_EW_log.loc[:datePP].Return_EW, \n",
    "                 mode='lines', \n",
    "                 name=f'Before {datetime.strftime(datePP,\"%Y/%m/%d\")} (datePP)',\n",
    "                 ))\n",
    "\n",
    "fig.update_layout(\n",
    "    # title=f'Return of an equally weighted portfolio of the 14 assets (including cryptos and traditional assets)',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Return EW portolio'\n",
    "    )\n",
    "\n",
    "fig.write_image(\"figures/part2a_ew_portfolio.png\") # Requires to install ! pip install -U kaleido\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Daily log-return equally weighted portfolio of the 14 assets (including cryptos and traditional assets)'\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot using Matplotlib for the report\n",
    "\n",
    "# Plot the returns of the portfolio EW\n",
    "plt.figure(figsize=(20, 6))\n",
    "\n",
    "plt.plot(portfolio_EW_log.loc[dateTr:dateRec].index, \n",
    "         portfolio_EW_log.loc[dateTr:dateRec].Return_EW, \n",
    "         label=f'Before {datetime.strftime(dateRec,\"%Y/%m/%d\")} (dateRec)', \n",
    "         color='black')\n",
    "\n",
    "plt.plot(portfolio_EW_log.loc[datePP:dateTr].index, \n",
    "         portfolio_EW_log.loc[datePP:dateTr].Return_EW, \n",
    "         label=f'Before {datetime.strftime(dateTr,\"%Y/%m/%d\")} (dateTr)', \n",
    "         color='dimgray')\n",
    "\n",
    "plt.plot(portfolio_EW_log.loc[:datePP].index, \n",
    "         portfolio_EW_log.loc[:datePP].Return_EW, \n",
    "         label=f'Before {datetime.strftime(datePP,\"%Y/%m/%d\")} (datePP)', \n",
    "         color='darkblue')\n",
    "\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Return EW portfolio')\n",
    "plt.savefig(\"figures/part2a_ew_portfolio.png\")\n",
    "plt.title('Daily log-return equally weighted portfolio of the 14 assets (including cryptos and traditional assets)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.b) Covariance matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using information from Lopez de Prado 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prado 2016: need 0.5*N*(N-1)IID observations to get non singular variance covariance matrix of size N. Here: N=14\n",
    "N = len(DATA_LOG_RETURN.columns)\n",
    "size = int(round(0.5*N*(N-1), 0))\n",
    "print(f\"Use {size} days of data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Variance covariance matrix - on {datetime.strftime(datePP, '%Y/%m/%d')} (dateTr)\")\n",
    "cov_matrix_datePP = DATA_LOG_RETURN.loc[datePP-timedelta(size):datePP].cov()\n",
    "cov_matrix_datePP.to_latex(\"tables/part2b_cov_matrix_datePP.tex\")\n",
    "save_large_tables(cov_matrix_datePP, \"tables/part2b_cov_matrix_datePP\")\n",
    "display(cov_matrix_datePP)\n",
    "\n",
    "print(f\"Variance covariance matrix - on {datetime.strftime(dateTr, '%Y/%m/%d')} (datePP)\")\n",
    "cov_matrix_dateTr = DATA_LOG_RETURN.loc[dateTr-timedelta(size):dateTr].cov()\n",
    "cov_matrix_dateTr.to_latex(\"tables/part2b_cov_matrix_dateTr.tex\")\n",
    "save_large_tables(cov_matrix_dateTr, \"tables/part2b_cov_matrix_dateTr\")\n",
    "display(cov_matrix_dateTr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test: might be totally wrong to do that: plot a heat map of the difference of covariance between the two dates\n",
    "diff_covariance = round(cov_matrix_datePP - cov_matrix_dateTr, 4)\n",
    "\n",
    "# Before datePP\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(diff_covariance, annot=True, cmap='coolwarm')\n",
    "plt.savefig(\"figures/part2b_heatmap_difference_covariance\")\n",
    "plt.title('Covariance difference Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.c) Clean the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to get the threshold to clean\n",
    "def determine_threshold(cov_matrix):\n",
    "\n",
    "    eigenvalues, _ = np.linalg.eigh(cov_matrix)\n",
    "    #Sort\n",
    "    eigenvalues_sorted = np.sort(eigenvalues)[::-1]\n",
    "    #Set threshold at value where eigenvalues appear to level off\n",
    "    threshold_index = np.where(eigenvalues_sorted < 0.25)[0][0]  \n",
    "    # print(threshold_index)\n",
    "    threshold = eigenvalues_sorted[threshold_index]\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the covariance matrix\n",
    "def clean_covariance_matrix(cov_matrix, threshold):\n",
    "\n",
    "    volatilities = np.sqrt(np.diag(cov_matrix))\n",
    "    correlation_matrix = cov_matrix / np.outer(volatilities, volatilities)\n",
    "\n",
    "    # Spectral decomposition to obtain eigenvalues and eigenvectors of correlation matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(correlation_matrix)\n",
    "\n",
    "    # print(eigenvalues)\n",
    "\n",
    "    # Clip the eigenvalues\n",
    "    clipped_eigenvalues = np.maximum(eigenvalues, threshold)\n",
    "    # print(clipped_eigenvalues)\n",
    "    diagonal_matrix = np.diag(clipped_eigenvalues)\n",
    "    cleaned_corr = eigenvectors @ diagonal_matrix @ eigenvectors.T\n",
    "\n",
    "    # Reconstruct the covariance matrix \n",
    "    cleaned_covariance_matrix= np.outer(volatilities, volatilities) * cleaned_corr\n",
    "    \n",
    "    return cleaned_covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Date PP\n",
    "volatilities_PP = np.sqrt(np.diag(cov_matrix_datePP))\n",
    "correlation_matrix_PP = cov_matrix_datePP / np.outer(volatilities_PP, volatilities_PP)\n",
    "\n",
    "thres_PP = determine_threshold(correlation_matrix_PP)\n",
    "print('Threshold for date PP:', thres_PP)\n",
    "\n",
    "cleaned_covariance_matrix_datePP = clean_covariance_matrix(cov_matrix_datePP, thres_PP)\n",
    "cleaned_covariance_matrix_datePP_df = pd.DataFrame(cleaned_covariance_matrix_datePP, index=cov_matrix_datePP.index, columns=cov_matrix_datePP.columns)\n",
    "\n",
    "print('\\nThe difference:')\n",
    "display(cov_matrix_datePP-cleaned_covariance_matrix_datePP) # there are differences\n",
    "\n",
    "#Date Tr\n",
    "volatilities_Tr = np.sqrt(np.diag(cov_matrix_dateTr))\n",
    "correlation_matrix_Tr = cov_matrix_dateTr / np.outer(volatilities_Tr, volatilities_Tr)\n",
    "\n",
    "thres_Tr = determine_threshold(correlation_matrix_Tr)\n",
    "print('\\nThreshold for date Tr:', thres_Tr)\n",
    "\n",
    "cleaned_covariance_matrix_dateTr = clean_covariance_matrix(cov_matrix_dateTr, thres_Tr)\n",
    "cleaned_covariance_matrix_dateTr_df = pd.DataFrame(cleaned_covariance_matrix_dateTr, index=cov_matrix_dateTr.index, columns=cov_matrix_dateTr.columns)\n",
    "\n",
    "print('The difference:')\n",
    "display(cov_matrix_dateTr-cleaned_covariance_matrix_dateTr)\n",
    "\n",
    "# Save tables\n",
    "cleaned_covariance_matrix_datePP_df.to_latex(\"tables/part2c_cleaned_covariance_matrix_datePP.tex\")\n",
    "cleaned_covariance_matrix_dateTr_df.to_latex(\"tables/part2c_cleaned_covariance_matrix_dateTr.tex\")\n",
    "save_large_tables(cleaned_covariance_matrix_datePP_df, \"tables/part2c_cleaned_covariance_matrix_datePP\")    # For the report\n",
    "save_large_tables(cleaned_covariance_matrix_dateTr_df, \"tables/part2c_cleaned_covariance_matrix_dateTr\")    # Report friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3D visualizations of raw and cleaned covariance matrices at datePP\n",
    "\n",
    "fig1 = plt.figure(figsize=(8, 6))\n",
    "ax1 = fig1.add_subplot(111, projection='3d')\n",
    "X1, Y1 = np.meshgrid(np.arange(len(cov_matrix_datePP)), np.arange(len(cov_matrix_datePP)))\n",
    "ax1.plot_surface(X1, Y1, cov_matrix_datePP, cmap='viridis')\n",
    "ax1.set_xlabel('Asset Index')\n",
    "ax1.set_ylabel('Asset Index')\n",
    "ax1.set_zlabel('Covariance')\n",
    "fig1.savefig(\"figures/part2c_raw_cov_matrix_datePP\")\n",
    "ax1.set_title('Raw Covariance Matrix (datePP)')\n",
    "\n",
    "fig2 = plt.figure(figsize=(8, 6))\n",
    "ax2 = fig2.add_subplot(111, projection='3d')\n",
    "X2, Y2 = np.meshgrid(np.arange(len(cleaned_covariance_matrix_datePP)), np.arange(len(cleaned_covariance_matrix_datePP)))\n",
    "ax2.plot_surface(X2, Y2, cleaned_covariance_matrix_datePP, cmap='viridis')\n",
    "ax2.set_xlabel('Asset Index')\n",
    "ax2.set_ylabel('Asset Index')\n",
    "ax2.set_zlabel('Covariance')\n",
    "fig2.savefig(\"figures/part2c_cleaned_cov_matrix_datePP\")\n",
    "ax2.set_title('Cleaned Covariance Matrix (datePP)')\n",
    "\n",
    "# Plot 3D visualizations of raw and cleaned covariance matrices at dateTr\n",
    "fig3 = plt.figure(figsize=(8, 6))\n",
    "ax3 = fig3.add_subplot(111, projection='3d')\n",
    "X3, Y3 = np.meshgrid(np.arange(len(cov_matrix_dateTr)), np.arange(len(cov_matrix_dateTr)))\n",
    "ax3.plot_surface(X3, Y3, cov_matrix_dateTr, cmap='viridis')\n",
    "ax3.set_xlabel('Asset Index')\n",
    "ax3.set_ylabel('Asset Index')\n",
    "ax3.set_zlabel('Covariance')\n",
    "fig3.savefig(\"figures/part2c_raw_cov_matrix_dateTr\")\n",
    "ax3.set_title('Raw Covariance Matrix (dateTr)')\n",
    "\n",
    "fig4 = plt.figure(figsize=(8, 6))\n",
    "ax4 = fig4.add_subplot(111, projection='3d')\n",
    "X4, Y4 = np.meshgrid(np.arange(len(cleaned_covariance_matrix_dateTr)), np.arange(len(cleaned_covariance_matrix_dateTr)))\n",
    "ax4.plot_surface(X4, Y4, cleaned_covariance_matrix_dateTr, cmap='viridis')\n",
    "ax4.set_xlabel('Asset Index')\n",
    "ax4.set_ylabel('Asset Index')\n",
    "ax4.set_zlabel('Covariance')\n",
    "fig4.savefig(\"figures/part2c_cleaned_cov_matrix_dateTr\")\n",
    "ax4.set_title('Cleaned Covariance Matrix (dateTr)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_eigen_spectrum(eigenvalues, title):\n",
    "    density = gaussian_kde(eigenvalues)\n",
    "    xs = np.linspace(min(eigenvalues), max(eigenvalues), 1000)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(xs, density(xs), label='Eigen-spectrum', linewidth=2)\n",
    "    plt.title(title, fontsize=12)\n",
    "    plt.xlabel('Eigenvalue', fontsize=10)\n",
    "    plt.ylabel('Density', fontsize=10)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def compute_condition_number(cov_matrix):\n",
    "    eigenvalues = np.linalg.eigvalsh(cov_matrix)\n",
    "    max_eigenvalue = np.max(eigenvalues)\n",
    "    min_eigenvalue = np.min(eigenvalues)\n",
    "    condition_number = max_eigenvalue / min_eigenvalue\n",
    "    return condition_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot kernel density estimates of eigen-spectra\n",
    "plot_eigen_spectrum_PP = plot_eigen_spectrum(np.linalg.eigvalsh(cov_matrix_datePP), 'Eigen-spectrum of Raw Covariance Matrix (datePP)')\n",
    "plot_eigen_spectrum_PP2 = plot_eigen_spectrum(np.linalg.eigvalsh(cleaned_covariance_matrix_datePP), 'Eigen-spectrum of Cleaned Covariance Matrix (datePP)')\n",
    "plot_eigen_spectrum_Tr = plot_eigen_spectrum(np.linalg.eigvalsh(cov_matrix_dateTr), 'Eigen-spectrum of Raw Covariance Matrix (dateTr)')\n",
    "plot_eigen_spectrum_Tr2 = plot_eigen_spectrum(np.linalg.eigvalsh(cleaned_covariance_matrix_dateTr), 'Eigen-spectrum of Cleaned Covariance Matrix (dateTr)')\n",
    "\n",
    "\n",
    "# Compare condition numbers\n",
    "raw_condition_number_PP = compute_condition_number(cov_matrix_datePP)\n",
    "cleaned_condition_number_PP = compute_condition_number(cleaned_covariance_matrix_datePP)\n",
    "raw_condition_number_Tr = compute_condition_number(cov_matrix_dateTr)\n",
    "cleaned_condition_number_Tr = compute_condition_number(cleaned_covariance_matrix_dateTr)\n",
    "\n",
    "print(\"Condition number of Raw Covariance Matrix (datePP):\", raw_condition_number_PP)\n",
    "print(\"Condition number of Cleaned Covariance Matrix (datePP):\", cleaned_condition_number_PP)\n",
    "print(\"Condition number of Raw Covariance Matrix (dateTr):\", raw_condition_number_Tr)\n",
    "print(\"Condition number of Cleaned Covariance Matrix (dateTr):\", cleaned_condition_number_Tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.d) Euler risk contribution structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Euler risk contribution method is a way to decompose the total risk of a portfolio into the risk contributed by each individual asset within the portfolio. It's based on the first-order Taylor expansion of the portfolio's total risk with respect to changes in the weights of individual assets.\n",
    "\n",
    "The total risk of a portfolio is typically measured by its standard deviation or variance. This quantifies how much the portfolio's value is expected to fluctuate.\n",
    "\n",
    "The partial derivatives of the portfolio's standard deviation or variance with respect to the weights of individual assets quantify the sensitivity of the portfolio's risk to changes in the weights of those assets. These derivatives tell us how much the portfolio's risk would change if we adjust the weight of a particular asset while keeping the weights of other assets constant.\n",
    "\n",
    "To calculate the risk contribution of each asset, we multiply the weight of each asset by its respective partial derivative. This gives us the contribution of each asset to the total risk of the portfolio.\n",
    "\n",
    "The Herfindahl index of the risk contributions provides a single number that summarizes the distribution of risk within the portfolio. It's calculated by summing the squares of the individual asset risk contributions and then dividing by the square of the portfolio standard deviation. A higher Herfindahl index indicates that risk is more concentrated in a few assets, while a lower index indicates more evenly distributed risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_euler_risk_contributions(weights, cov_matrix):\n",
    "    portfolio_std = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    partial_sigma_partial_w = (cov_matrix @ weights) / portfolio_std\n",
    "    euler_risk_contributions = weights * partial_sigma_partial_w\n",
    "    return euler_risk_contributions\n",
    "\n",
    "def calculate_herfindahl_index(euler_risk_contributions):\n",
    "    return np.sum(np.square(euler_risk_contributions)) / np.square(np.sum(euler_risk_contributions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of the 14 assets in the portfolio is allocated an equal weight of 1/14 \n",
    "weights_date_PP = np.array([1/14] * 14)  \n",
    "weights_date_Tr = np.array([1/14] * 14)\n",
    "\n",
    "\n",
    "# Calculate Euler risk contributions for both portfolios at both dates using raw sample covariances\n",
    "euler_rc_date_PP_raw = calculate_euler_risk_contributions(weights_date_PP, cov_matrix_datePP)\n",
    "euler_rc_date_Tr_raw = calculate_euler_risk_contributions(weights_date_Tr, cov_matrix_dateTr)\n",
    "\n",
    "\n",
    "# Calculate Euler risk contributions for both portfolios at both dates using cleaned covariances\n",
    "euler_rc_date_PP_cleaned = calculate_euler_risk_contributions(weights_date_PP, cleaned_covariance_matrix_datePP)\n",
    "euler_rc_date_Tr_cleaned = calculate_euler_risk_contributions(weights_date_Tr, cleaned_covariance_matrix_dateTr)\n",
    "\n",
    "\n",
    "# Calculate Herfindahl index for each portfolio and each estimator\n",
    "herfindahl_date_PP_raw = calculate_herfindahl_index(euler_rc_date_PP_raw)\n",
    "herfindahl_date_Tr_raw = calculate_herfindahl_index(euler_rc_date_Tr_raw)\n",
    "herfindahl_date_PP_cleaned = calculate_herfindahl_index(euler_rc_date_PP_cleaned)\n",
    "herfindahl_date_Tr_cleaned = calculate_herfindahl_index(euler_rc_date_Tr_cleaned)\n",
    "\n",
    "print(herfindahl_date_PP_raw)\n",
    "print(herfindahl_date_Tr_raw)\n",
    "print(herfindahl_date_PP_cleaned)\n",
    "print(herfindahl_date_Tr_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.e) Diversification distribution of the portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_decomposition(correlation_matrix):\n",
    "    # Compute the eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(correlation_matrix)\n",
    "\n",
    "    # Sort the eigenvalues in descending order\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "    eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "\n",
    "    # The diagonal matrix Delta containing the eigenvalues\n",
    "    A= np.diag(eigenvalues_sorted)\n",
    "\n",
    "    # E is the matrix of eigenvectors corresponding to the sorted eigenvalues\n",
    "    E = eigenvectors_sorted\n",
    "    return(E,A,eigenvalues_sorted)\n",
    "\n",
    "def diversification_distribution(weights,eigenvalues):\n",
    "    var=0\n",
    "    l=len(eigenvalues)\n",
    "    List_probability_distribution=[]\n",
    "    for i in range(l):\n",
    "        var+=(weights[i]**2)*eigenvalues[i]\n",
    "    for i in range(l):\n",
    "        List_probability_distribution.append((weights[i]**2)*eigenvalues[i]/var)\n",
    "    return(List_probability_distribution)\n",
    "\n",
    "def Effective_Number_of_Bets(N,K,weights,eigenvalues):\n",
    "    S=0\n",
    "    L=diversification_distribution(weights,eigenvalues)\n",
    "    for i in range(K-1,N):\n",
    "        S+=L[i]*np.log(L[i])\n",
    "    return(np.exp(-S),L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Diversification Distribution Date_PP_raw\n",
    "eigenvalues_sorted_PP_raw = principal_component_decomposition(cov_matrix_datePP)[2]\n",
    "l = len(eigenvalues_sorted_PP_raw)\n",
    "weights=[weights_EW for i in range(l)]\n",
    "Effective_Number_of_Bets_PP_raw = Effective_Number_of_Bets(l, 1, weights, eigenvalues_sorted_PP_raw)[0]\n",
    "data_plot_PP_raw = {'x': [i for i in range(1, l+1)],'y': Effective_Number_of_Bets(l, 1, weights, eigenvalues_sorted_PP_raw)[1]}\n",
    "df_plot_PP_raw = pd.DataFrame(data_plot_PP_raw)\n",
    "df_plot_PP_raw\n",
    "# Create the plot\n",
    "sns.set_theme()  # Optional: applies the default seaborn theme styling\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "barplot = sns.barplot(x='x', y='y', data=df_plot_PP_raw, color='red', saturation=0.7)\n",
    "# Adding the dashed line at y=1\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "for index, row in df_plot_PP_raw.iterrows():\n",
    "    barplot.text(row['x'] - 1, row['y'], f'{row[\"y\"]:.2f}', color='black', ha='center', va='bottom')\n",
    "# Set labels (optional)\n",
    "plt.title('Diversification Distribution Date_PP_raw')\n",
    "plt.xlabel('principal portfolios')\n",
    "plt.ylabel('diversification distribution')\n",
    "\n",
    "## 2. Diversification Distribution Date_Tr_raw\n",
    "eigenvalues_sorted_Tr_raw=principal_component_decomposition(cov_matrix_dateTr)[2]\n",
    "Effective_Number_of_Bets_Tr_raw=Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_Tr_raw)[0]\n",
    "data_plot_Tr_raw = {'x': [i for i in range(1,l+1)],'y': Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_Tr_raw)[1]}\n",
    "df_plot_Tr_raw= pd.DataFrame(data_plot_Tr_raw)\n",
    "# Create the plot\n",
    "sns.set_theme()  # Optional: applies the default seaborn theme styling\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "barplot = sns.barplot(x='x', y='y', data=df_plot_Tr_raw, color='red', saturation=0.7)\n",
    "# Adding the dashed line at y=1\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "for index, row in df_plot_Tr_raw.iterrows():\n",
    "    barplot.text(row['x'] - 1, row['y'], f'{row[\"y\"]:.2f}', color='black', ha='center', va='bottom')\n",
    "# Set labels (optional)\n",
    "plt.title('Diversification Distribution Date_Tr_raw')\n",
    "plt.xlabel('principal portfolios')\n",
    "plt.ylabel('diversification distribution')\n",
    "\n",
    "## 3. Diversification Distribution Date_PP_cleaned\n",
    "eigenvalues_sorted_PP_cleaned=principal_component_decomposition(cleaned_covariance_matrix_datePP)[2]\n",
    "l=len(eigenvalues_sorted_PP_cleaned)\n",
    "weights=[weights_EW for i in range(l)]\n",
    "Effective_Number_of_Bets_PP_cleaned=Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_PP_cleaned)[0]\n",
    "data_plot_PP_cleaned = {'x': [i for i in range(1,l+1)],'y': Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_PP_cleaned)[1]}\n",
    "df_plot_PP_cleaned= pd.DataFrame(data_plot_PP_cleaned)\n",
    "# Create the plot\n",
    "sns.set_theme()  # Optional: applies the default seaborn theme styling\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "barplot = sns.barplot(x='x', y='y', data=df_plot_PP_cleaned, color='red', saturation=0.7)\n",
    "# Adding the dashed line at y=1\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "for index, row in df_plot_PP_cleaned.iterrows():\n",
    "    barplot.text(row['x'] - 1, row['y'], f'{row[\"y\"]:.2f}', color='black', ha='center', va='bottom')\n",
    "# Set labels (optional)\n",
    "plt.title('Diversification Distribution Date_PP_cleaned')\n",
    "plt.xlabel('principal portfolios')\n",
    "plt.ylabel('diversification distribution')\n",
    "\n",
    "## 3. Diversification Distribution Date_Tr_cleaned\n",
    "eigenvalues_sorted_Tr_cleaned=principal_component_decomposition(cleaned_covariance_matrix_dateTr)[2]\n",
    "Effective_Number_of_Bets_Tr_cleaned=Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_Tr_cleaned)[0]\n",
    "data_plot_Tr_cleaned= {'x': [i for i in range(1,l+1)],'y': Effective_Number_of_Bets(l,1,weights,eigenvalues_sorted_Tr_cleaned)[1]}\n",
    "df_plot_Tr_cleaned= pd.DataFrame(data_plot_Tr_cleaned)\n",
    "# Create the plot\n",
    "sns.set_theme()  # Optional: applies the default seaborn theme styling\n",
    "plt.figure(figsize=(10, 6))  # Set the figure size\n",
    "barplot = sns.barplot(x='x', y='y', data=df_plot_Tr_cleaned, color='red', saturation=0.7)\n",
    "# Adding the dashed line at y=1\n",
    "plt.axhline(1, color='black', linestyle='--')\n",
    "for index, row in df_plot_Tr_cleaned.iterrows():\n",
    "    barplot.text(row['x'] - 1, row['y'], f'{row[\"y\"]:.2f}', color='black', ha='center', va='bottom')\n",
    "# Set labels (optional)\n",
    "plt.title('Diversification Distribution Date_Tr_cleaned')\n",
    "plt.xlabel('principal portfolios')\n",
    "plt.ylabel('diversification distribution')\n",
    "\n",
    "\n",
    "plt.show()\n",
    "print('Effective_Number_of_Bets_PP_raw =',Effective_Number_of_Bets_PP_raw)\n",
    "print('Effective_Number_of_Bets_Tr_raw =',Effective_Number_of_Bets_Tr_raw)\n",
    "print('Effective_Number_of_Bets_PP_cleaned =',Effective_Number_of_Bets_PP_cleaned)\n",
    "print('Effective_Number_of_Bets_Tr_cleaned =',Effective_Number_of_Bets_Tr_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_by_values(data):\n",
    "    # Convert the data to a pandas Series\n",
    "    crypto_returns = pd.Series(data)\n",
    "    \n",
    "    # Create a DataFrame with the value and calculate the rank\n",
    "    crypto_with_values = crypto_returns.to_frame(name='Value')\n",
    "    crypto_with_values['Rank'] = crypto_with_values['Value'].rank(method='average', ascending=False)\n",
    "    \n",
    "    # Sort by rank\n",
    "    sorted_crypto_with_values = crypto_with_values.sort_values(by='Rank')\n",
    "    \n",
    "    # Reset index to show the name as a column\n",
    "    sorted_crypto_with_values.reset_index(inplace=True)\n",
    "    sorted_crypto_with_values.rename(columns={'index': 'Name'}, inplace=True)\n",
    "    \n",
    "    return sorted_crypto_with_values[['Rank', 'Name', 'Value']]\n",
    "\n",
    "from scipy.stats import kendalltau\n",
    "def kendall_tau_correlation_matrix(data):\n",
    "    n = data.shape[1]\n",
    "    corr_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                corr_matrix[i, j] = 1\n",
    "            elif i < j:\n",
    "                tau, _ = kendalltau(data[:, i], data[:, j])\n",
    "                corr_matrix[i, j] = tau\n",
    "                corr_matrix[j, i] = tau\n",
    "    return corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asset=['ADA','BCH' ,'BTC','DOGE', 'ETH','LINK','LTC','MANA','XLM','XRP','SPXT','XCMP','USSOC','VIX']\n",
    "euler_rc_date_Tr_raw_df=pd.DataFrame({'Name':asset,'risk':euler_rc_date_Tr_raw})\n",
    "euler_rc_date_Tr_cleaned_df=pd.DataFrame({'Name':asset,'risk':euler_rc_date_Tr_cleaned})\n",
    "\n",
    "df_TR_raw=rank_by_values(euler_rc_date_Tr_raw_df.reset_index(drop=True).set_index('Name')['risk'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Name', y='Value', data=df_TR_raw,color='red')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.title('Values of risk contribution dateTr(raw)')\n",
    "plt.tight_layout() \n",
    "\n",
    "df_TR_cleaned=rank_by_values(euler_rc_date_Tr_cleaned_df.reset_index(drop=True).set_index('Name')['risk'])\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Name', y='Value', data=df_TR_cleaned,color='red')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.title('Values of risk contribution dateTr(cleaned)')\n",
    "plt.tight_layout() \n",
    "\n",
    "df=rank_by_values(portfolio_EW_log.loc[dateTr][:-1])\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = sns.barplot(x='Name', y='Value', data=df,color='red')\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.title('Return losses per asset')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kendall's Tau rank order correlation matrix \n",
    "\n",
    "data_raw = np.array([portfolio_EW_log.loc[dateTr][:-1].tolist(),euler_rc_date_Tr_raw_df.reset_index(drop=True).set_index('Name')['risk'].tolist()]).T  # Transpose to align lists as columns\n",
    "# Compute the Kendall's Tau correlation matrix\n",
    "corr_matrix = kendall_tau_correlation_matrix(data_raw)\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(corr_matrix, interpolation='none', cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(data_raw.shape[1]), ['portfolio_EW', 'euler_rc_date_Tr_raw'])\n",
    "plt.yticks(range(data_raw.shape[1]), ['portfolio_EW', 'euler_rc_date_Tr_raw'])\n",
    "plt.title('Kendall’s Tau Correlation Matrix_raw')\n",
    "\n",
    "\n",
    "data_cleanedKD = np.array([portfolio_EW_log.loc[dateTr][:-1].tolist(),euler_rc_date_Tr_cleaned_df.reset_index(drop=True).set_index('Name')['risk'].tolist()]).T  # Transpose to align lists as columns\n",
    "# Compute the Kendall's Tau correlation matrix\n",
    "corr_matrix = kendall_tau_correlation_matrix(data_cleanedKD)\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(corr_matrix, interpolation='none', cmap='coolwarm', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.xticks(range(data_raw.shape[1]), ['portfolio_EW', 'euler_rc_date_cleaned_raw'])\n",
    "plt.yticks(range(data_raw.shape[1]), ['portfolio_EW', 'euler_rc_date_cleaned_raw'])\n",
    "plt.title('Kendall’s Tau Correlation Matrix_cleaned')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Risk based portfolios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_COV_PP = cleaned_covariance_matrix_datePP_df\n",
    "VAR_COV_TR = cleaned_covariance_matrix_dateTr_df\n",
    "RETURNS_PP = data_cleaned.loc[:datePP]\n",
    "RETURNS_TR = data_cleaned.loc[datePP:dateTr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some useful functions for the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_return_portfolio(weights, returns):\n",
    "    \"\"\"Computes the return of the portfolio.\n",
    "\n",
    "    Args:\n",
    "        weights (dict): weigths of each assets in the portfolio: {asset: weight}\n",
    "        returns (dataframe): matrix of returns\n",
    "    \"\"\"\n",
    "    key = list(weights.keys())\n",
    "    col = returns.columns.to_list()\n",
    "\n",
    "    for i in range(len(weights)): # make sure columns match\n",
    "        if not key[i] == col[i]:\n",
    "            raise ValueError(f\"There is an issue with the order of the keys of the weights and the data. They must have the same sequence. Weights: {key}, dataframe: {col}\")\n",
    "    \n",
    "    mean_vec = returns.mean()\n",
    "    \n",
    "    ret = sum([weights[stock] * mean_vec[stock] for stock in weights.keys()])\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_variance_portfolio(weights, covariance):\n",
    "    \"\"\"Computes the variance of the portfolio.\n",
    "\n",
    "    Args:\n",
    "        weights (dict): weigths of each assets in the portfolio: {asset: weight}\n",
    "        covariance (dataframe): matrix of variance-covariance\n",
    "    \"\"\"\n",
    "    key = list(weights.keys())\n",
    "    col = covariance.columns.to_list()\n",
    "\n",
    "    for i in range(len(weights)): # make sure columns match\n",
    "        if not key[i] == col[i]:\n",
    "            raise ValueError(f\"There is an issue with the order of the keys of the weights and the data. They must have the same sequence. Weights: {key}, dataframe: {col}\")\n",
    "    \n",
    "    values = np.array(list(weights.values()))\n",
    "    \n",
    "    var = values@covariance@values.T\n",
    "    \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_portfolio_weights(weights, title=None):\n",
    "    \"\"\"Displays the weights from a dictionnary\"\"\"\n",
    "    if title:\n",
    "        print(title)\n",
    "        for (k,v) in enumerate(weights):\n",
    "            print(f\"- {v}\\t: {round(weights[v], 4)}\")\n",
    "\n",
    "def save_portfolio_weights(weights, title, date, rounding = 4):\n",
    "    weights_df = pd.DataFrame.from_dict(weights, orient='index', columns=['Weights'])\n",
    "    weights_df.Weights = weights_df.Weights.apply(lambda row: str(round(row, rounding)))\n",
    "    weights_df.to_latex(f\"{title}_{date}.tex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.a) Portfolio creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.a) i. Minimum variance portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the portfolio variance\n",
    "def portfolio_variance(weights, covariance_matrix):\n",
    "    return weights.T @ covariance_matrix @ weights\n",
    "\n",
    "# Number of assets\n",
    "num_assets = VAR_COV_PP.shape[0]  \n",
    "\n",
    "# Constraints for the optimization (weights sum to 1 and non-negative weights)\n",
    "constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})  # The weights must sum to 1\n",
    "bounds = tuple((0, 1) for _ in range(num_assets))               # Non-negative weights for each asset\n",
    "\n",
    "# Initial guess (equal weights)\n",
    "initial_weights = np.ones(num_assets) / num_assets\n",
    "\n",
    "# Portfolio optimization for datePP\n",
    "result_datePP = minimize(portfolio_variance, initial_weights, args=(VAR_COV_PP,),\n",
    "                         method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "weights_datePP_min_var = result_datePP.x\n",
    "weights_datePP_min_var_dic = {VAR_COV_PP.columns.to_list()[i]:result_datePP.x[i] for i in range(num_assets)}\n",
    "\n",
    "# Portfolio optimization for dateTr\n",
    "result_dateTr = minimize(portfolio_variance, initial_weights, args=(VAR_COV_TR,),\n",
    "                         method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "weights_dateTr_min_var = result_dateTr.x\n",
    "\n",
    "weights_dateTr_min_var_dic = {VAR_COV_PP.columns.to_list()[i]:result_dateTr.x[i] for i in range(num_assets)}\n",
    "\n",
    "\n",
    "# Display results\n",
    "# print(\"Minimum Variance Portfolio Weights at DatePP:\", weights_datePP_min_var)\n",
    "# print(\"Minimum Variance Portfolio Weights at DateTr:\", weights_dateTr_min_var)\n",
    "\n",
    "save_portfolio_weights(weights_datePP_min_var_dic, \"tables/part3ai_weights_min_var\", \"datePP\")\n",
    "save_portfolio_weights(weights_dateTr_min_var_dic, \"tables/part3ai_weights_min_var\", \"dateTr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_datePP_min_var_dic, \"Minimum Variance Portfolio Weights at DatePP:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_dateTr_min_var_dic, \"Minimum Variance Portfolio Weights at DateTr:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute return and variances of the 2 portfolios\n",
    "return_min_var_datePP = get_return_portfolio(weights_datePP_min_var_dic, RETURNS_TR)\n",
    "return_min_var_dateTr = get_return_portfolio(weights_dateTr_min_var_dic, RETURNS_TR)\n",
    "var_min_var_datePP =    get_variance_portfolio(weights_datePP_min_var_dic, VAR_COV_PP)\n",
    "var_min_var_dateTr =    get_variance_portfolio(weights_dateTr_min_var_dic, VAR_COV_TR)\n",
    "\n",
    "print(f\"Minimum variance portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(return_min_var_datePP, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_min_var_datePP ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"Minimum variance portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(return_min_var_dateTr, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_min_var_dateTr ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.a) ii. Equal risk contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk contribution function\n",
    "def risk_contribution(weights, covariance_matrix):\n",
    "    total_portfolio_variance = np.dot(weights.T, np.dot(covariance_matrix, weights))\n",
    "    marginal_contributions = np.dot(covariance_matrix, weights)\n",
    "    risk_contributions = weights * marginal_contributions / total_portfolio_variance\n",
    "    return risk_contributions\n",
    "\n",
    "def objective_function(weights, covariance_matrix):\n",
    "    target_risk_contribution = np.ones(num_assets) / num_assets\n",
    "    actual_risk_contributions = risk_contribution(weights, covariance_matrix)\n",
    "    return np.sum((actual_risk_contributions - target_risk_contribution) ** 2)\n",
    "\n",
    "# Portfolio optimization for datePP\n",
    "result_erc_datePP = minimize(objective_function, initial_weights, args=(VAR_COV_PP,),\n",
    "                             method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "weights_erc_datePP = result_erc_datePP.x\n",
    "\n",
    "# Weights to dict\n",
    "weights_erc_datePP_dic = {VAR_COV_PP.columns.to_list()[i]:result_erc_datePP.x[i] for i in range(num_assets)}\n",
    "\n",
    "# Portfolio optimization for dateTr\n",
    "result_erc_dateTr = minimize(objective_function, initial_weights, args=(VAR_COV_TR,),\n",
    "                             method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "weights_erc_dateTr = result_erc_dateTr.x\n",
    "\n",
    "# Weights to dict\n",
    "weights_erc_dateTr_dic = {VAR_COV_PP.columns.to_list()[i]:result_erc_dateTr.x[i] for i in range(num_assets)}\n",
    "\n",
    "# Display results\n",
    "# print(\"Equal Risk Contribution Portfolio Weights at DatePP:\\n\", weights_erc_datePP)\n",
    "# print(\"\\nEqual Risk Contribution Portfolio Weights at DateTr:\\n\", weights_erc_dateTr)\n",
    "\n",
    "save_portfolio_weights(weights_erc_datePP_dic, \"tables/part3aii_weights_erc\", \"datePP\")\n",
    "save_portfolio_weights(weights_erc_dateTr_dic, \"tables/part3aii_weights_erc\", \"dateTr\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_erc_datePP_dic, \"Equal Risk Contribution Portfolio Weights at DatePP:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_erc_dateTr_dic, \"Equal Risk Contribution Portfolio Weights at DateTr:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute return and variances of the 2 portfolios\n",
    "return_erc_datePP = get_return_portfolio(weights_erc_datePP_dic, RETURNS_TR)\n",
    "return_erc_dateTr = get_return_portfolio(weights_erc_dateTr_dic, RETURNS_TR)\n",
    "var_erc_datePP =    get_variance_portfolio(weights_erc_datePP_dic, VAR_COV_PP)\n",
    "var_erc_dateTr =    get_variance_portfolio(weights_erc_dateTr_dic, VAR_COV_TR)\n",
    "\n",
    "print(f\"Equal Risk Contribution portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(return_erc_datePP, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(return_erc_dateTr ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"Equal Risk Contribution portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(var_erc_datePP, 4)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_erc_dateTr ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.a) iii. Minimum effective number of bets portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S is the Covariance Matrix \n",
    "#A is the Conditional Matrix\n",
    "#E is the Matrix of the eigenvectors\n",
    "#L is the List of the eigenvalues\n",
    "\n",
    "def real_parts(complex_list):\n",
    "    return [z.real for z in complex_list]\n",
    "\n",
    "def gen_first_eig_vect(S, A):\n",
    "    N = S.shape[0]\n",
    "    P = np.eye(N)\n",
    "    if np.linalg.matrix_rank(A) > 0:\n",
    "        P = np.eye(N) - A.T @ np.linalg.inv(A @ A.T) @ A\n",
    "    L, E = np.linalg.eig(P @ S @ P.T)\n",
    "    \n",
    "    indexx = np.argmax(L)\n",
    "    e = E[:, indexx]\n",
    "\n",
    "    return e\n",
    "\n",
    "\n",
    "def gen_pcbasis(S, A):      #Compute the conditional principal portfolios.\n",
    "    if A.size == 0:\n",
    "        N = S.shape[0]\n",
    "        K = 0\n",
    "        E_, L_ = np.linalg.eig(S)\n",
    "        idx = np.argsort(-np.diag(L_))\n",
    "        E = E_[:, idx]\n",
    "        L = np.diag(L_)[idx]\n",
    "    else:\n",
    "        K, N = A.shape\n",
    "        E = np.empty((N, 0))\n",
    "        B = A\n",
    "        for n in range(N-K):\n",
    "            if E.shape[1] > 0:\n",
    "                B = np.vstack([A, E.T @ S])\n",
    "            e = gen_first_eig_vect(S, B)\n",
    "            E = np.hstack([E, e[:, np.newaxis]])\n",
    "\n",
    "        for n in range(N-K, N):\n",
    "            B = E.T @ S\n",
    "            e = gen_first_eig_vect(S, B)\n",
    "            E = np.hstack([E, e[:, np.newaxis]])\n",
    "\n",
    "        # Swap order\n",
    "        E = np.hstack([E[:, N-K:], E[:, :N-K]])\n",
    "\n",
    "    L = np.diag(E.T @ S @ E)\n",
    "    G = np.diag(np.sqrt(L)) @ np.linalg.inv(E)\n",
    "    if K > 0:\n",
    "        G = G[K:, :]\n",
    "\n",
    "    return E, L, G\n",
    "\n",
    "def diversification_distribution_conditional(weights,eigenvalues,k):\n",
    "    var=0\n",
    "    l=len(eigenvalues)\n",
    "    List_probability_distribution=[]\n",
    "    for i in range(k,l):\n",
    "        var+=(weights[i]**2)*eigenvalues[i]\n",
    "    for i in range(k,l):\n",
    "        List_probability_distribution.append((weights[i]**2)*eigenvalues[i]/var)\n",
    "    return(List_probability_distribution)\n",
    "\n",
    "def Effective_Number_of_Bets_conditional(K,E,weights,eigenvalues):\n",
    "    S=0\n",
    "    weights=np.linalg.inv(E)@weights\n",
    "    L=diversification_distribution_conditional(weights,eigenvalues,K)\n",
    "    N=len(L)\n",
    "    for i in range(N):\n",
    "        S+=L[i]*np.log(L[i])\n",
    "    return(np.exp(-S))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E,L,G=gen_pcbasis(VAR_COV_TR,np.ones((1,l)))\n",
    "N=len(L)\n",
    "def constraint(weights):\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "def objective_function(weights):\n",
    "    return(Effective_Number_of_Bets_conditional(1,E,weights,L))\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = [1/N for i in range(N)]\n",
    "\n",
    "# Bounds for each weight (non-negativity constraint)\n",
    "bounds = [(0, None)] * N\n",
    "\n",
    "# Constraints\n",
    "constraints = [{'type': 'eq', 'fun': constraint}]\n",
    "\n",
    "# Minimize entropy subject to constraints\n",
    "result = minimize(objective_function, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "\n",
    "# Dictionary of weights\n",
    "weights_minimum_effective_number_of_bets_TR={}\n",
    "for i in range(14):\n",
    "    weights_minimum_effective_number_of_bets_TR[asset[i]]=result.x[i]\n",
    "\n",
    "E,L,G=gen_pcbasis(VAR_COV_PP,np.ones((1,l)))\n",
    "N=len(L)\n",
    "def constraint(weights):\n",
    "    return np.sum(weights) - 1\n",
    "\n",
    "def objective_function(weights):\n",
    "    return(Effective_Number_of_Bets_conditional(1,E,weights,L))\n",
    "\n",
    "# Initial guess\n",
    "initial_guess = [1/N for i in range(N)]\n",
    "\n",
    "# Bounds for each weight (non-negativity constraint)\n",
    "bounds = [(0, None)] * N\n",
    "\n",
    "# Constraints\n",
    "constraints = [{'type': 'eq', 'fun': constraint}]\n",
    "\n",
    "# Minimize entropy subject to constraints\n",
    "result = minimize(objective_function, initial_guess, method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "# Dictionary of weights\n",
    "weights_minimum_effective_number_of_bets_PP={}\n",
    "for i in range(14):\n",
    "    weights_minimum_effective_number_of_bets_PP[asset[i]]=result.x[i]\n",
    "\n",
    "# Save the weights\n",
    "save_portfolio_weights(weights_minimum_effective_number_of_bets_PP, \"tables/part3aiii_weights_min_eff_nb_bets\", \"datePP\")\n",
    "save_portfolio_weights(weights_minimum_effective_number_of_bets_TR, \"tables/part3aiii_weights_min_eff_nb_bets\", \"dateTr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_minimum_effective_number_of_bets_PP, \"Minimum effecitve number of bets portfolio Weights at DatePP:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp_portfolio_weights(weights_minimum_effective_number_of_bets_TR, \"Minimum effecitve number of bets portfolio Weights at DateTr:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute return and variances of the 2 portfolios\n",
    "return_minimum_effective_number_of_bets_PP = get_return_portfolio(weights_minimum_effective_number_of_bets_PP, RETURNS_PP)\n",
    "return_minimum_effective_number_of_bets_Tr = get_return_portfolio(weights_minimum_effective_number_of_bets_TR, RETURNS_TR)\n",
    "variance_minimum_effective_number_of_bets_PP =    get_variance_portfolio(weights_minimum_effective_number_of_bets_PP, VAR_COV_PP)\n",
    "variance_minimum_effective_number_of_bets_Tr =    get_variance_portfolio(weights_minimum_effective_number_of_bets_TR, VAR_COV_TR)\n",
    "\n",
    "print(f\"Equal Risk Contribution portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(return_minimum_effective_number_of_bets_PP, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(variance_minimum_effective_number_of_bets_PP ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"Equal Risk Contribution portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(return_minimum_effective_number_of_bets_Tr, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(variance_minimum_effective_number_of_bets_Tr ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3.a) iv. Hierarchical risk portolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HRP_portolio import *\n",
    "\n",
    "def get_HRP(data, covariance, show = None):\n",
    "\n",
    "    # Compute distance matrix\n",
    "    volatilities = np.sqrt(np.diag(covariance))\n",
    "    correlation = covariance / np.outer(volatilities, volatilities)\n",
    "    correlation = data.corr() # comment when correlation matrix is symmetric\n",
    "    distance_matrix = correlation.apply(lambda row: (0.5*(1-row))**0.5)\n",
    "\n",
    "    # Get ordered distance matrix, cluster order and linkage matrix (uses gmarti.gitlab.io)\n",
    "    ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(distance_matrix.values, method='single')\n",
    "\n",
    "    # Formatting of the dataframe for clearer representation\n",
    "    ordered_dist_mat_df = pd.DataFrame(\n",
    "        ordered_dist_mat, \n",
    "        index = list(correlation.columns), \n",
    "        columns=list(correlation.columns)\n",
    "\n",
    "    )\n",
    "\n",
    "    res_linkage_df = pd.DataFrame(\n",
    "        res_linkage, \n",
    "        index = [f\"Cluster {i+1}\" for i in range(res_linkage.shape[0])], \n",
    "        columns= [\"Elem 1\", \"Elem 2\", \"Distance\", \"Original nb elem cluster\"]\n",
    "    )\n",
    "\n",
    "    if show:\n",
    "        print(\"\\nOrdered distance matrix\")\n",
    "        display(ordered_dist_mat_df)\n",
    "\n",
    "        print(\"\\nLinkage matrix\")\n",
    "        display(res_linkage_df)\n",
    "\n",
    "        print(\"\\nRes order\")\n",
    "        print(res_order)\n",
    "        \n",
    "\n",
    "    return ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cluster_HRP(res_linkage, save=False):\n",
    "    \"\"\" Displays the dendrogram of the clusters of the assets\n",
    "        Input: res_linkage\n",
    "    \"\"\"\n",
    "    scipy.cluster.hierarchy.dendrogram(res_linkage)\n",
    "    if save:\n",
    "        plt.savefig(\"figures/part3aiv_dendrogram_HRP\")\n",
    "\n",
    "    plt.title(\"Hierarchical cluster of the assets\")\n",
    "    plt.show()\n",
    "\n",
    "# show_cluster_HRP(res_linkage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_HRP_weights(covariance, res_order, show = False):\n",
    "\n",
    "    # The input matrice needs to have numerical indices and column names\n",
    "    c = covariance.copy()\n",
    "    c.reset_index(drop=True, inplace = True)\n",
    "    c.columns = list(c.index) \n",
    "\n",
    "    HRP_weights = compute_HRP_weights(\n",
    "        c,\n",
    "        res_order\n",
    "    ).sort_index()\n",
    "\n",
    "    HRP_weights_dic = {\n",
    "        list(covariance.columns)[i]:HRP_weights.loc[i] for i in range(len(HRP_weights))\n",
    "        }\n",
    "\n",
    "    if show:\n",
    "\n",
    "        disp_portfolio_weights(HRP_weights_dic, title=\"Hierarchical risk portfolio weights:\")\n",
    "        \n",
    "        # print(\"Hierarchical risk portfolio weights:\")\n",
    "        # for (k,v) in enumerate(HRP_weights_dic):\n",
    "        #     print(f\"- {v}\\t: {round(HRP_weights_dic[v], 4)}\")\n",
    "\n",
    "    return HRP_weights_dic\n",
    "\n",
    "    \n",
    "# w = get_HRP_weights(VAR_COV_PP, res_order, show=True)#covariance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HRP(data, covariance, show=False):\n",
    "\n",
    "    ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order = get_HRP(data=data, covariance = covariance, show=show)\n",
    "    \n",
    "    if show:\n",
    "        show_cluster_HRP(res_linkage_df)\n",
    "\n",
    "    weights = get_HRP_weights(covariance = covariance, res_order = res_order, show = show)\n",
    "\n",
    "    return weights, ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order\n",
    "\n",
    "# weights = HRP(data = DATA_LOG_RETURN, covariance = VAR_COV_PP, show = True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last function that allows to display all results of HRP for a given date, and saving tables\n",
    "def run_HRP(data, covariance, date, save=False, rounding=4, show=False):\n",
    "\n",
    "    if show:\n",
    "        print(f\"-- Hierarchical risk parity portfolio results for date {date} --\")\n",
    "\n",
    "    # Run the HRP function to get all the informations\n",
    "    weights, ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order = HRP(data = data, covariance = covariance, show = show)\n",
    "\n",
    "    if save:\n",
    "\n",
    "        # Save weights\n",
    "        HRP_weights_df_latex = pd.DataFrame.from_dict(weights, orient='index', columns=['Weights'])\n",
    "        HRP_weights_df_latex.Weights = HRP_weights_df_latex.Weights.apply(lambda row: str(round(row, rounding)))\n",
    "        HRP_weights_df_latex.to_latex(f\"tables/part3aiv_HRP_weights_{date}.tex\")\n",
    "\n",
    "        # Save ordered_dist_mat_df\n",
    "        ordered_dist_mat_df = ordered_dist_mat_df.round(rounding).to_latex(f\"tables/part3aiv_HRP_ordered_distance_matrix_{date}.tex\")\n",
    "\n",
    "        # Save distance_matrix\n",
    "        distance_matrix = distance_matrix.round(rounding).to_latex(f\"tables/part3aiv_HRP_distance_matrix_{date}.tex\")\n",
    "\n",
    "        # Save res_linkage_df\n",
    "        res_linkage_df[\"Elem 1\"] = res_linkage_df[\"Elem 1\"].astype(int)\n",
    "        res_linkage_df[\"Elem 2\"] = res_linkage_df[\"Elem 2\"].astype(int)\n",
    "        res_linkage_df[\"Original nb elem cluster\"] = res_linkage_df[\"Original nb elem cluster\"].astype(int)\n",
    "        res_linkage_df[\"Distance\"] = res_linkage_df[\"Distance\"].round(rounding)\n",
    "        res_linkage_df = res_linkage_df.to_latex(f\"tables/part3aiv_HRP_res_linkage_{date}.tex\")\n",
    "\n",
    "\n",
    "    return weights, ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order\n",
    "\n",
    "# run_HRP(data = DATA_LOG_RETURN, covariance = VAR_COV_PP, date = \"datePP\", show = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the HRP on both dates\n",
    "\n",
    "# Data for datePP and dateTr\n",
    "HRP_data_datePP = DATA_LOG_RETURN[DATA_LOG_RETURN.index < datePP]\n",
    "HRP_data_dateTr = DATA_LOG_RETURN[DATA_LOG_RETURN.index < dateTr]\n",
    "HRP_data_dateTr = HRP_data_dateTr[HRP_data_dateTr.index > datePP]\n",
    "\n",
    "# As of datePP\n",
    "weights_HRP_datePP_dic, ordered_dist_mat_df_datePP, res_linkage_df_datePP, distance_matrix_datePP, res_order_datePP = run_HRP(data = HRP_data_datePP, covariance = VAR_COV_PP, date = \"datePP\", show = True, save = True)\n",
    "print(\"\\n\")\n",
    "\n",
    "# As of dateTr\n",
    "weights_HRP_dateTr_dic, ordered_dist_mat_df_dateTr, res_linkage_df_dateTr, distance_matrix_dateTr, res_order_dateTr = run_HRP(data = HRP_data_dateTr, covariance = VAR_COV_TR, date = \"dateTr\", show = True, save = True)\n",
    "\n",
    "#weights_datePP, ordered_dist_mat_df_datePP, res_linkage_df_datePP, distance_matrix_datePP, res_order_datePP = HRP(data = DATA_LOG_RETURN, covariance = VAR_COV_PP, show = True)\n",
    "\n",
    "# weights_datePP, ordered_dist_mat_df_datePP, res_linkage_df_datePP, distance_matrix_datePP, res_order_datePP = run_HRP(data = DATA_LOG_RETURN, covariance = VAR_COV_PP, date = \"datePP\", show = True, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute return and variances of the 2 portfolios\n",
    "return_HRP_datePP = get_return_portfolio(weights_HRP_datePP_dic, RETURNS_PP)\n",
    "return_HRP_dateTr = get_return_portfolio(weights_HRP_dateTr_dic, RETURNS_TR)\n",
    "var_HRP_datePP =    get_variance_portfolio(weights_HRP_datePP_dic, VAR_COV_PP)\n",
    "var_HRP_dateTr =    get_variance_portfolio(weights_HRP_dateTr_dic, VAR_COV_TR)\n",
    "\n",
    "print(f\"HRP portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(return_HRP_datePP, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_HRP_datePP ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"HRP portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(return_HRP_dateTr, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_HRP_dateTr ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot distance matrix - Evidence of seriation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy example to show the seriation of the distance matrix\n",
    "correlation = HRP_data_datePP.corr()\n",
    "distance_matrix = correlation.apply(lambda row: (0.5*(1-row))**0.5)\n",
    "\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(distance_matrix.values, method='single')\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(distance_matrix, cmap='coolwarm')\n",
    "plt.savefig(\"figures/part3aiv_distance_matrix\")\n",
    "plt.title('Distance matrix')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(ordered_dist_mat,cmap='coolwarm')\n",
    "plt.savefig(\"figures/part3aiv_ordered_distance_matrix\")\n",
    "plt.title('Ordered distance matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Extensions to Hierarchical Risk Parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust Nonparametric Copula Based Dependence Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$C_N(\\frac{i}{N}, \\frac{j}{N}) = \\frac{1}{N} [\\# \\text{ of } (X_k^1, X_k^2) \\text{ st } X_k^1 <= X_i^1 \\text{ and } X_k^2<= X_i^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute the empirical CDF using rank data\n",
    "def empirical_cdf(x):\n",
    "    return x.rank(method='max').values / len(x)\n",
    "\n",
    "def get_SW_sigma_estimator(df):\n",
    "\n",
    "    # Number of observations\n",
    "    N = len(df)\n",
    "\n",
    "    # Get the list of variable names\n",
    "    variables = df.columns\n",
    "\n",
    "    # Initialize the matrix to store the SW estimators\n",
    "    sigma_matrix = pd.DataFrame(index=variables, columns=variables)\n",
    "\n",
    "    # Compute the empirical CDFs for all variables and convert to numpy array\n",
    "    cdf_data = df.apply(empirical_cdf).to_numpy()\n",
    "\n",
    "    # Calculate the Schweizer-Wolff sigma estimator for all pairs of variables\n",
    "    for i in tqdm(range(len(variables))):\n",
    "        for j in range(i, len(variables)):\n",
    "            if i == j:\n",
    "                sigma_matrix.iloc[i, j] = 0\n",
    "            else:\n",
    "                # Get empirical CDFs for the pair of variables\n",
    "                F1 = cdf_data[:, i]\n",
    "                F2 = cdf_data[:, j]\n",
    "\n",
    "                # Construct the empirical copula matrix\n",
    "                empirical_copula = np.zeros((N, N))\n",
    "                for k in range(N):\n",
    "                    empirical_copula[k] = np.mean((F1[:, None] <= (k + 1) / N) & (F2[:, None] <= (np.arange(N) + 1) / N), axis=0)\n",
    "\n",
    "                # Calculate the Schweizer-Wolff sigma estimator\n",
    "                outer_product = np.outer(np.arange(1, N + 1) / N, np.arange(1, N + 1) / N)\n",
    "                sigma = 12 / (N ** 2 - 1) * np.sum(np.abs(empirical_copula - outer_product))\n",
    "                sigma_matrix.iloc[i, j] = sigma\n",
    "                sigma_matrix.iloc[j, i] = sigma\n",
    "    \n",
    "    return sigma_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HRP_portolio import *\n",
    "\n",
    "# Last function that allows to display all results of HRP for a given date, and saving tables\n",
    "def run_HRP_SW(data, sigmaSW, date, save=False, rounding=4):\n",
    "    # Get the CDF\n",
    "    cdf_data = data.apply(empirical_cdf).to_numpy()\n",
    "    \n",
    "    # Run the HRP function to get all the informations\n",
    "    seriated_dist, res_order, res_linkage = compute_serial_matrix(sigmaSW.values)\n",
    "    HRP_SW_weights = compute_HRP_weights(pd.DataFrame(cdf_data), res_order)\n",
    "    HRP_SW_weights.sort_index()\n",
    "\n",
    "    HRP_SW_weights = {list(data)[i]:HRP_SW_weights.loc[i] for i in range(len(HRP_SW_weights))}\n",
    "    \n",
    "    #weights, ordered_dist_mat_df, res_linkage_df, distance_matrix, res_order = HRP(data = data, covariance = sigmaSW, show = show)\n",
    "\n",
    "    ordered_dist_mat_df = pd.DataFrame(\n",
    "            seriated_dist, \n",
    "            index = list(data.columns), \n",
    "            columns=list(data.columns)   \n",
    "        )\n",
    "\n",
    "    if save:\n",
    "\n",
    "        # Save weights\n",
    "        HRP_SW_weights_df_latex = pd.DataFrame.from_dict(HRP_SW_weights, orient='index', columns=['Weights'])\n",
    "        HRP_SW_weights_df_latex.Weights = HRP_SW_weights_df_latex.Weights.apply(lambda row: str(round(row, rounding)))\n",
    "        HRP_SW_weights_df_latex.to_latex(f\"tables/part4_HRP_SW_weights_{date}.tex\")\n",
    "\n",
    "        # Save ordered_dist_mat_df\n",
    "        ordered_dist_mat_df = ordered_dist_mat_df.round(rounding).to_latex(f\"tables/part4_HRP_SW_ordered_distance_matrix_{date}.tex\")\n",
    "\n",
    "        res_linkage_df = pd.DataFrame(\n",
    "            res_linkage, \n",
    "            index = [f\"Cluster {i+1}\" for i in range(res_linkage.shape[0])], \n",
    "            columns= [\"Elem 1\", \"Elem 2\", \"Distance\", \"Original nb elem cluster\"]\n",
    "        )\n",
    "        res_linkage_df[\"Elem 1\"] = res_linkage_df[\"Elem 1\"].astype(int)\n",
    "        res_linkage_df[\"Elem 2\"] = res_linkage_df[\"Elem 2\"].astype(int)\n",
    "        res_linkage_df[\"Original nb elem cluster\"] = res_linkage_df[\"Original nb elem cluster\"].astype(int)\n",
    "        res_linkage_df[\"Distance\"] = res_linkage_df[\"Distance\"].round(rounding)\n",
    "        res_linkage_df = res_linkage_df.to_latex(f\"tables/part4_HRP_SW_res_linkage_{date}.tex\")\n",
    "\n",
    "\n",
    "    return HRP_SW_weights, ordered_dist_mat_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_Tr = get_SW_sigma_estimator(RETURNS_TR)\n",
    "sigma_PP = get_SW_sigma_estimator(RETURNS_PP)\n",
    "\n",
    "# sigma_Tr.to_csv(\"sigma_Tr.csv\")\n",
    "# sigma_PP.to_csv(\"sigma_PP.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HRP SW date PP\n",
      "- ADA\t: 0.0078\n",
      "- BCH\t: 0.0106\n",
      "- BTC\t: 0.0376\n",
      "- DOGE\t: 0.0194\n",
      "- ETH\t: 0.011\n",
      "- LINK\t: 0.0256\n",
      "- LTC\t: 0.0427\n",
      "- MANA\t: 0.0157\n",
      "- XLM\t: 0.038\n",
      "- XRP\t: 0.6566\n",
      "- SPXT\t: 0.0441\n",
      "- XCMP\t: 0.022\n",
      "- USSOC\t: 0.0245\n",
      "- VIX\t: 0.0445\n",
      "\n",
      "HRP SW date Tr\n",
      "- ADA\t: 0.0078\n",
      "- BCH\t: 0.0106\n",
      "- BTC\t: 0.0376\n",
      "- DOGE\t: 0.0194\n",
      "- ETH\t: 0.011\n",
      "- LINK\t: 0.0256\n",
      "- LTC\t: 0.0427\n",
      "- MANA\t: 0.0157\n",
      "- XLM\t: 0.038\n",
      "- XRP\t: 0.6566\n",
      "- SPXT\t: 0.0441\n",
      "- XCMP\t: 0.022\n",
      "- USSOC\t: 0.0245\n",
      "- VIX\t: 0.0445\n"
     ]
    }
   ],
   "source": [
    "# sigma_Tr = pd.read_csv(\"sigma_Tr.csv\", index_col=[0])\n",
    "# sigma_PP = pd.read_csv(\"sigma_PP.csv\", index_col=[0])\n",
    "\n",
    "weights_HRP_SW_datePP_dic, seriated_dist_Tr = run_HRP_SW(RETURNS_TR, sigma_Tr, \"dateTr\")\n",
    "weights_HRP_SW_dateTr_dic, seriated_dist_PP = run_HRP_SW(RETURNS_TR, sigma_Tr, \"datePP\")\n",
    "\n",
    "disp_portfolio_weights(weights_HRP_SW_datePP_dic, title=\"HRP SW date PP\")\n",
    "disp_portfolio_weights(weights_HRP_SW_dateTr_dic, title=\"\\nHRP SW date Tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute return and variances of the 2 portfolios\n",
    "return_HRP_SW_datePP = get_return_portfolio(weights_HRP_SW_datePP_dic, RETURNS_PP)\n",
    "return_HRP_SW_dateTr = get_return_portfolio(weights_HRP_SW_dateTr_dic, RETURNS_TR)\n",
    "var_HRP_SW_datePP =    get_variance_portfolio(weights_HRP_SW_datePP_dic, VAR_COV_PP)\n",
    "var_HRP_SW_dateTr =    get_variance_portfolio(weights_HRP_SW_dateTr_dic, VAR_COV_TR)\n",
    "\n",
    "print(f\"Equal Risk Contribution portfolio as of datePP\")\n",
    "print(f\"  - Expected return: \\t {round(return_HRP_SW_datePP, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_HRP_SW_datePP ** 0.5, 4)}%\")\n",
    "print()\n",
    "print(f\"Equal Risk Contribution portfolio as of dateTr\")\n",
    "print(f\"  - Expected return: \\t {round(return_HRP_SW_dateTr, 5)}%\")\n",
    "print(f\"  - Volatility: \\t {round(var_HRP_SW_dateTr ** 0.5, 4)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigma_Tr.to_csv(\"sigma_Tr.csv\")\n",
    "# sigma_PP.to_csv(\"sigma_PP.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
